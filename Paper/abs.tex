Testing approximate software for non-deterministic hardware is different from regular testing in that good tests may show inherently flaky behavior. I.e., an approximate program (run on approximate hardware) may have unacceptable output occasionally and still be bug-free. Approximate computing tests, therefore, will likely need to be re-run multiple times in order to generate an output distribution which we can compare to an estimate of what the output distribution should be. Our solution for testing approximate programs for non-deterministic hardware is to rerun regular tests many times on emulated non-deterministic hardware. We propose comparing the test results with an estimate of how the known (because it was emulated) non-determinism should have affected the outputs. By detecting outliers (using Huber statistics or LMS detection) in the result distribution we can identify potential bugs.
